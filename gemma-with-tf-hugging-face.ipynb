{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10261,"sourceType":"modelInstanceVersion","modelInstanceId":5388},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318},{"sourceId":11270,"sourceType":"modelInstanceVersion","modelInstanceId":6216},{"sourceId":11384,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":6216},{"sourceId":11382,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma Model 2B Model (2 Billion Parameter Base Model) - Hugging Face TF ","metadata":{}},{"cell_type":"markdown","source":"# Import Hugging Face \n## What does the `transformers` library do?\n\nThe `transformers` library, developed by Hugging Face, is one of the most popular and widely used libraries for natural language processing (NLP) and deep learning tasks related to text data. Here's an overview of what it does:\n\n1. **Pre-trained Models**: Transformers provides a wide range of pre-trained models for various NLP tasks such as text classification, named entity recognition, question answering, language translation, and more. These models are trained on large datasets and can be fine-tuned on specific tasks with relatively little labeled data, which is known as transfer learning.\n\n2. **Model Architecture**: The library implements state-of-the-art transformer-based architectures such as BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), RoBERTa, DistilBERT, T5, and more. These architectures have significantly advanced the field of NLP by achieving impressive results on benchmark datasets.\n\n3. **Tokenization**: Transformers offers tokenization utilities to convert raw text into numerical inputs that can be fed into the models. This includes handling tasks like splitting text into words or subwords, mapping tokens to their corresponding indices, and adding special tokens required for tasks like classification or generation.\n\n4. **Model Training and Inference**: The library provides tools for training and fine-tuning models on custom datasets. It includes utilities for data loading, model configuration, training loop management, evaluation metrics, and model serialization. Additionally, it offers inference capabilities for deploying trained models to make predictions on new data.\n\n5. **Integration with Deep Learning Frameworks**: Transformers is compatible with popular deep learning frameworks such as TensorFlow and PyTorch, allowing users to seamlessly integrate transformer-based models into their existing workflows.\n\n6. **Community and Resources**: The Transformers library has a large and active community of users and contributors who provide support, share resources, and contribute to the development of new features and models. The library is well-documented with tutorials, examples, and API references to help users get started with NLP tasks.\n\nOverall, the Transformers library is a powerful tool for researchers, developers, and data scientists working on NLP tasks, offering a wide range of pre-trained models, easy-to-use APIs, and extensive documentation.\n","metadata":{}},{"cell_type":"markdown","source":"Enable GPU (GPI P100)\n\nHere I use, GPU P100 because not much parellel computing is required.","metadata":{}},{"cell_type":"code","source":"!pip install -U git+https://github.com/huggingface/transformers  accelerate bitsandbytes #pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:02.896356Z","iopub.execute_input":"2024-02-26T17:55:02.897701Z","iopub.status.idle":"2024-02-26T17:55:36.679501Z","shell.execute_reply.started":"2024-02-26T17:55:02.897667Z","shell.execute_reply":"2024-02-26T17:55:36.678335Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-zxevwpxt\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-zxevwpxt\n  Resolved https://github.com/huggingface/transformers to commit 3b8c053631a2088d74fbb6ef4db47dbed8fa1470\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.42.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:36.681554Z","iopub.execute_input":"2024-02-26T17:55:36.681879Z","iopub.status.idle":"2024-02-26T17:55:36.688076Z","shell.execute_reply.started":"2024-02-26T17:55:36.681848Z","shell.execute_reply":"2024-02-26T17:55:36.687210Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"A. Create Quantization Configuration","metadata":{}},{"cell_type":"code","source":"\nquantization_config = BitsAndBytesConfig(load_in_8bit=False) #load_in_4bit","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:36.689393Z","iopub.execute_input":"2024-02-26T17:55:36.690588Z","iopub.status.idle":"2024-02-26T17:55:36.710010Z","shell.execute_reply.started":"2024-02-26T17:55:36.690556Z","shell.execute_reply":"2024-02-26T17:55:36.709190Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"B. Download Model (Using Version 2 Gemma 2TB)\n\n*Add the directory path \n*For the 7B model, please use the commented after #\n","metadata":{}},{"cell_type":"code","source":"\nmodel_id = \"/kaggle/input/gemma/transformers/2b-it/2\" #\"gg-hf/gemma-2b-it\" #/kaggle/input/gemma/transformers/7b-it/1\ndtype = torch.float16","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:36.711900Z","iopub.execute_input":"2024-02-26T17:55:36.712834Z","iopub.status.idle":"2024-02-26T17:55:36.720262Z","shell.execute_reply.started":"2024-02-26T17:55:36.712809Z","shell.execute_reply":"2024-02-26T17:55:36.719425Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",#move my model weight between CPU and gPU\n    torch_dtype=dtype,\n    quantization_config = quantization_config\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T17:55:36.721038Z","iopub.execute_input":"2024-02-26T17:55:36.721296Z","iopub.status.idle":"2024-02-26T17:55:57.253071Z","shell.execute_reply.started":"2024-02-26T17:55:36.721274Z","shell.execute_reply":"2024-02-26T17:55:57.252350Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a0d0dc6cc2497aa6b02e1662b8f055"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Option 1 (without Chat Template)\n1. Simply input the text \n2. Tokenization\n3. Generating Ouput\n4. Decoding","metadata":{}},{"cell_type":"code","source":"# Use the model\ninput_text = \"Who is the most popular CEO?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**input_ids, max_new_tokens=250)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:57.254085Z","iopub.execute_input":"2024-02-26T17:55:57.254353Z","iopub.status.idle":"2024-02-26T17:55:58.630353Z","shell.execute_reply.started":"2024-02-26T17:55:57.254330Z","shell.execute_reply":"2024-02-26T17:55:58.629552Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"<bos>Who is the most popular CEO?\n\nAccording to Forbes, the most popular CEO is Elon Musk, with an estimated net worth of $292 billion.<eos>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Option 2 (with chat template)","metadata":{}},{"cell_type":"markdown","source":"Uploading a correct chat templates improves the performance of LLM","metadata":{}},{"cell_type":"code","source":"\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a joke about Elon Musk\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:58.631492Z","iopub.execute_input":"2024-02-26T17:55:58.631837Z","iopub.status.idle":"2024-02-26T17:55:58.644483Z","shell.execute_reply.started":"2024-02-26T17:55:58.631804Z","shell.execute_reply":"2024-02-26T17:55:58.643570Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"prompt","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:58.645704Z","iopub.execute_input":"2024-02-26T17:55:58.646335Z","iopub.status.idle":"2024-02-26T17:55:58.656937Z","shell.execute_reply.started":"2024-02-26T17:55:58.646303Z","shell.execute_reply":"2024-02-26T17:55:58.656069Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"'<start_of_turn>user\\nWrite a joke about Elon Musk<end_of_turn>\\n<start_of_turn>model\\n'"},"metadata":{}}]},{"cell_type":"code","source":"inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:58.657874Z","iopub.execute_input":"2024-02-26T17:55:58.658098Z","iopub.status.idle":"2024-02-26T17:55:59.569008Z","shell.execute_reply.started":"2024-02-26T17:55:58.658078Z","shell.execute_reply":"2024-02-26T17:55:59.568268Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T17:55:59.571552Z","iopub.execute_input":"2024-02-26T17:55:59.571826Z","iopub.status.idle":"2024-02-26T17:55:59.576941Z","shell.execute_reply.started":"2024-02-26T17:55:59.571803Z","shell.execute_reply":"2024-02-26T17:55:59.576008Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nWrite a joke about Elon Musk<end_of_turn>\n<start_of_turn>model\nWhat do you call Elon Musk's new car?\n\nA Tesla Autopilot!<eos>\n","output_type":"stream"}]}]}